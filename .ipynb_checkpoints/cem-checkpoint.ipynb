{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-15 15:29:31,341] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Iteration  0. Episode mean reward:  20.455\n",
      "Iteration  1. Episode mean reward:  53.888\n",
      "Iteration  2. Episode mean reward: 107.483\n",
      "Iteration  3. Episode mean reward: 161.607\n",
      "Iteration  4. Episode mean reward: 184.425\n",
      "Iteration  5. Episode mean reward: 191.695\n",
      "Iteration  6. Episode mean reward: 193.685\n",
      "Iteration  7. Episode mean reward: 195.037\n",
      "Iteration  8. Episode mean reward: 195.692\n",
      "Iteration  9. Episode mean reward: 197.135\n",
      "Iteration 10. Episode mean reward: 199.030\n",
      "Iteration 11. Episode mean reward: 197.393\n",
      "Iteration 12. Episode mean reward: 199.072\n",
      "Iteration 13. Episode mean reward: 199.218\n",
      "Iteration 14. Episode mean reward: 199.882\n",
      "Iteration 15. Episode mean reward: 199.905\n",
      "Iteration 16. Episode mean reward: 199.028\n",
      "Iteration 17. Episode mean reward: 199.610\n",
      "Iteration 18. Episode mean reward: 199.898\n",
      "Iteration 19. Episode mean reward: 199.755\n",
      "Iteration 20. Episode mean reward: 199.815\n",
      "Iteration 21. Episode mean reward: 199.895\n",
      "Iteration 22. Episode mean reward: 199.820\n",
      "Iteration 23. Episode mean reward: 199.993\n",
      "Iteration 24. Episode mean reward: 199.940\n",
      "Iteration 25. Episode mean reward: 199.912\n",
      "Iteration 26. Episode mean reward: 199.887\n",
      "Iteration 27. Episode mean reward: 199.985\n",
      "Iteration 28. Episode mean reward: 200.000\n",
      "Iteration 29. Episode mean reward: 199.977\n",
      "Iteration 30. Episode mean reward: 199.525\n",
      "Iteration 31. Episode mean reward: 200.000\n",
      "Iteration 32. Episode mean reward: 200.000\n",
      "Iteration 33. Episode mean reward: 200.000\n",
      "Iteration 34. Episode mean reward: 199.947\n",
      "Iteration 35. Episode mean reward: 200.000\n",
      "Iteration 36. Episode mean reward: 200.000\n",
      "Iteration 37. Episode mean reward: 200.000\n",
      "Iteration 38. Episode mean reward: 200.000\n",
      "Iteration 39. Episode mean reward: 200.000\n",
      "Iteration 40. Episode mean reward: 200.000\n",
      "Iteration 41. Episode mean reward: 200.000\n",
      "Iteration 42. Episode mean reward: 199.963\n",
      "Iteration 43. Episode mean reward: 199.805\n",
      "Iteration 44. Episode mean reward: 199.988\n",
      "Iteration 45. Episode mean reward: 200.000\n",
      "Iteration 46. Episode mean reward: 199.900\n",
      "Iteration 47. Episode mean reward: 199.995\n",
      "Iteration 48. Episode mean reward: 200.000\n",
      "Iteration 49. Episode mean reward: 199.860\n",
      "Iteration 50. Episode mean reward: 199.998\n",
      "Iteration 51. Episode mean reward: 199.688\n",
      "Iteration 52. Episode mean reward: 199.780\n",
      "Iteration 53. Episode mean reward: 199.920\n",
      "Iteration 54. Episode mean reward: 199.825\n",
      "Iteration 55. Episode mean reward: 199.955\n",
      "Iteration 56. Episode mean reward: 199.982\n",
      "Iteration 57. Episode mean reward: 200.000\n",
      "Iteration 58. Episode mean reward: 200.000\n",
      "Iteration 59. Episode mean reward: 199.980\n",
      "Iteration 60. Episode mean reward: 199.952\n",
      "Iteration 61. Episode mean reward: 200.000\n",
      "Iteration 62. Episode mean reward: 200.000\n",
      "Iteration 63. Episode mean reward: 199.825\n",
      "Iteration 64. Episode mean reward: 199.935\n",
      "Iteration 65. Episode mean reward: 199.998\n",
      "Iteration 66. Episode mean reward: 199.963\n",
      "Iteration 67. Episode mean reward: 200.000\n",
      "Iteration 68. Episode mean reward: 199.803\n",
      "Iteration 69. Episode mean reward: 200.000\n",
      "Iteration 70. Episode mean reward: 199.938\n",
      "Iteration 71. Episode mean reward: 200.000\n",
      "Iteration 72. Episode mean reward: 200.000\n",
      "Iteration 73. Episode mean reward: 200.000\n",
      "Iteration 74. Episode mean reward: 200.000\n",
      "Iteration 75. Episode mean reward: 200.000\n",
      "Iteration 76. Episode mean reward: 200.000\n",
      "Iteration 77. Episode mean reward: 200.000\n",
      "Iteration 78. Episode mean reward: 200.000\n",
      "Iteration 79. Episode mean reward: 200.000\n",
      "Iteration 80. Episode mean reward: 200.000\n",
      "Iteration 81. Episode mean reward: 200.000\n",
      "Iteration 82. Episode mean reward: 200.000\n",
      "Iteration 83. Episode mean reward: 200.000\n",
      "Iteration 84. Episode mean reward: 200.000\n",
      "Iteration 85. Episode mean reward: 200.000\n",
      "Iteration 86. Episode mean reward: 200.000\n",
      "Iteration 87. Episode mean reward: 200.000\n",
      "Iteration 88. Episode mean reward: 200.000\n",
      "Iteration 89. Episode mean reward: 200.000\n",
      "Iteration 90. Episode mean reward: 200.000\n",
      "Iteration 91. Episode mean reward: 200.000\n",
      "Iteration 92. Episode mean reward: 200.000\n",
      "Iteration 93. Episode mean reward: 199.977\n",
      "Iteration 94. Episode mean reward: 199.977\n",
      "Iteration 95. Episode mean reward: 200.000\n",
      "Iteration 96. Episode mean reward: 200.000\n",
      "Iteration 97. Episode mean reward: 200.000\n",
      "Iteration 98. Episode mean reward: 200.000\n",
      "Iteration 99. Episode mean reward: 200.000\n",
      "[-0.17677851 -0.8941039  -1.94331048 -1.95996783  0.05254057]\n",
      "stay up time: 4.5266931057\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "class BinaryActionLinearPolicy(object):\n",
    "    def __init__(self, theta):\n",
    "        self.w = theta[:-1]\n",
    "        self.b = theta[-1]\n",
    "    def act(self, ob):\n",
    "        y = ob.dot(self.w) + self.b\n",
    "        a = int(y < 0)\n",
    "        return a\n",
    "\n",
    "class ContinuousActionLinearPolicy(object):\n",
    "    def __init__(self, theta, n_in, n_out):\n",
    "        assert len(theta) == (n_in + 1) * n_out\n",
    "        self.W = theta[0 : n_in * n_out].reshape(n_in, n_out)\n",
    "        self.b = theta[n_in * n_out : None].reshape(1, n_out)\n",
    "    def act(self, ob):\n",
    "        a = ob.dot(self.W) + self.b\n",
    "        return a\n",
    "\n",
    "def cem(f, th_mean, batch_size, n_iter, elite_frac, initial_std=1.0):\n",
    "    \"\"\"\n",
    "    Generic implementation of the cross-entropy method for maximizing a black-box function\n",
    "\n",
    "    f: a function mapping from vector -> scalar\n",
    "    th_mean: initial mean over input distribution\n",
    "    batch_size: number of samples of theta to evaluate per batch\n",
    "    n_iter: number of batches\n",
    "    elite_frac: each batch, select this fraction of the top-performing samples\n",
    "    initial_std: initial standard deviation over parameter vectors\n",
    "    \"\"\"\n",
    "    n_elite = int(np.round(batch_size*elite_frac))\n",
    "    th_std = np.ones_like(th_mean) * initial_std\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        ths = np.array([th_mean + dth for dth in  th_std[None,:]*np.random.randn(batch_size, th_mean.size)])\n",
    "        ys = np.array([f(th) for th in ths])\n",
    "        elite_inds = ys.argsort()[::-1][:n_elite]\n",
    "        elite_ths = ths[elite_inds]\n",
    "        th_mean = elite_ths.mean(axis=0)\n",
    "        th_std = elite_ths.std(axis=0)\n",
    "        yield {'ys' : ys, 'theta_mean' : th_mean, 'y_mean' : ys.mean()}\n",
    "        \n",
    "        \n",
    "def do_rollout(agent, env, render=False):\n",
    "    total_rew = 0\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    while done == False:\n",
    "        a = agent.act(ob)\n",
    "        (ob, reward, done, _info) = env.step(a)\n",
    "        total_rew += reward\n",
    "#         if render and t%3==0: env.render()\n",
    "        if done: break\n",
    "    return total_rew\n",
    "\n",
    "def noisy_evaluation(theta):\n",
    "    agent = BinaryActionLinearPolicy(theta)\n",
    "    rew = do_rollout(agent, env)\n",
    "    return rew\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "print env.action_space.n\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "params = dict(n_iter=100, batch_size=400, elite_frac = 0.2)\n",
    "\n",
    "for (i, iterdata) in enumerate(\n",
    "    cem(noisy_evaluation, np.zeros(env.observation_space.shape[0]+1), **params)):\n",
    "    print('Iteration %2i. Episode mean reward: %7.3f'%(i, iterdata['y_mean']))\n",
    "    agent = BinaryActionLinearPolicy(iterdata['theta_mean'])\n",
    "\n",
    "print iterdata['theta_mean']\n",
    "ob = env.reset()\n",
    "done = False\n",
    "start_t = time.time()\n",
    "while done == False:\n",
    "    a = agent.act(ob)\n",
    "    (ob, reward, done, _info) = env.step(a)\n",
    "    env.render()\n",
    "    if done: break\n",
    "end_t = time.time()    \n",
    "print('stay up time: '+str(end_t-start_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-15 14:40:03,736] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0 0.98011\n",
      "9.0 0.97022\n",
      "8.0 0.96033\n",
      "11.0 0.95044\n",
      "8.0 0.94055\n",
      "9.0 0.93066\n",
      "10.0 0.92077\n",
      "11.0 0.91088\n",
      "9.0 0.90099\n",
      "9.0 0.8911\n",
      "8.0 0.88121\n",
      "11.0 0.87132\n",
      "8.0 0.86143\n",
      "9.0 0.85154\n",
      "10.0 0.84165\n",
      "8.0 0.83176\n",
      "10.0 0.82187\n",
      "8.0 0.81198\n",
      "9.0 0.80209\n",
      "12.0 0.7922\n",
      "8.0 0.78231\n",
      "9.0 0.77242\n",
      "11.0 0.76253\n",
      "11.0 0.75264\n",
      "10.0 0.74275\n",
      "10.0 0.73286\n",
      "9.0 0.72297\n",
      "8.0 0.71308\n",
      "12.0 0.70319\n",
      "10.0 0.6933\n",
      "9.0 0.68341\n",
      "12.0 0.67352\n",
      "12.0 0.66363\n",
      "9.0 0.65374\n",
      "10.0 0.64385\n",
      "9.0 0.63396\n",
      "9.0 0.62407\n",
      "9.0 0.61418\n",
      "12.0 0.60429\n",
      "9.0 0.5944\n",
      "9.0 0.58451\n",
      "10.0 0.57462\n",
      "13.0 0.56473\n",
      "10.0 0.55484\n",
      "9.0 0.54495\n",
      "9.0 0.53506\n",
      "9.0 0.52517\n",
      "10.0 0.51528\n",
      "9.0 0.50539\n",
      "11.0 0.4955\n",
      "10.0 0.48561\n",
      "10.0 0.47572\n",
      "9.0 0.46583\n",
      "10.0 0.45594\n",
      "10.0 0.44605\n",
      "10.0 0.43616\n",
      "9.0 0.42627\n",
      "8.0 0.41638\n",
      "9.0 0.40649\n",
      "10.0 0.3966\n",
      "9.0 0.38671\n",
      "10.0 0.37682\n",
      "9.0 0.36693\n",
      "10.0 0.35704\n",
      "8.0 0.34715\n",
      "10.0 0.33726\n",
      "9.0 0.32737\n",
      "10.0 0.31748\n",
      "10.0 0.30759\n",
      "10.0 0.2977\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import cPickle as pickle\n",
    "import random\n",
    "from collections import deque\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "input_num =  4# number of input demension\n",
    "output_num = 2 # number of valid actions\n",
    "observe = 100 # timesteps to observe before training\n",
    "explore = 5000 # frames over which to anneal epsilon\n",
    "final_epsilon = 0.001 # final value of epsilon\n",
    "initial_epsilon = 0.99 # starting value of epsilon\n",
    "replay_memory = 5000 # number of previous transitions to remember\n",
    "\n",
    "gamma = 0.7 # decay rate of past observations\n",
    "\n",
    "mu = 0.0001\n",
    "delta = 1.75\n",
    "\n",
    "beta = 25\n",
    "alpha = 4.72\n",
    "    \n",
    "def Guassion_kernel(x, u):\n",
    "    x = np.matrix(x,dtype=np.float64).T\n",
    "    u = np.matrix(u,dtype=np.float64).T\n",
    "    return np.exp(-(np.dot((x-u).T, (x-u)))/(2*delta**2))\n",
    "\n",
    "\n",
    "    \n",
    "def qfunc(phi_x, action, w):\n",
    "    return np.dot(phi_x.T, w[:,action])[0,0]\n",
    "\n",
    "def greedy(phi_x, w):\n",
    "    q_value = np.dot(phi_x.T, w)\n",
    "    return np.argmax(q_value)\n",
    "\n",
    "def epsilon_greedy(phi_x, w, epsilon):\n",
    "    amax = greedy(phi_x, w)\n",
    "    if random.random() <= epsilon:\n",
    "        amax = random.randrange(2)           \n",
    "    return amax\n",
    "    \n",
    "def construct_Phi(x, ALD_buf):\n",
    "    Phi_x = []\n",
    "    for a in ALD_buf:\n",
    "        Phi_x.append(Guassion_kernel(x, a)[0,0])\n",
    "    return Phi_x\n",
    "\n",
    "\n",
    "def achieve_y(r_t, s_t1, done, w, ALD_buf):\n",
    "    if done:\n",
    "        return -1\n",
    "    else:\n",
    "        x = np.array(s_t1)\n",
    "        k_t = np.matrix(construct_Phi(x, ALD_buf), dtype=np.float64).T \n",
    "        y_t = np.dot(k_t.T,w)\n",
    "        y_max = np.max(y_t)\n",
    "        return 0.01 + gamma * y_max\n",
    "    \n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(0)\n",
    "high = env.observation_space.high\n",
    "low = env.observation_space.low\n",
    "\n",
    "def norm(x):\n",
    "    norm_x = np.ones_like(x)\n",
    "    for i in range(len(x)):\n",
    "        norm_x[i] = (x[i]-low[i])/(high[i]-low[i])\n",
    "    return norm_x\n",
    "\n",
    "np.random.seed(0)\n",
    "n_action = env.action_space.n\n",
    "num_steps = 200    \n",
    "epsilon = initial_epsilon\n",
    "# initial gpql\n",
    "s_t = env.reset()\n",
    "init_x = np.array(s_t)\n",
    "ALD_buf = []\n",
    "ALD_buf.append(init_x)\n",
    "k_tt = Guassion_kernel(init_x, init_x)\n",
    "K_inv = k_tt.I\n",
    "Pn = np.matrix((1/alpha)*np.eye(1))\n",
    "Mn = np.matrix(np.zeros([1,n_action]))\n",
    "m = 1\n",
    "D = deque()\n",
    "start_t = time.time()\n",
    "\n",
    "# s_t = norm(s_t)\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    done = False\n",
    "    reward = 0\n",
    "    while done == False:\n",
    "        k_t = np.matrix(construct_Phi(s_t, ALD_buf), dtype=np.float64).T\n",
    "        a = greedy(k_t, Mn)\n",
    "        (s_t1, r, done, _info) = env.step(a)\n",
    "#         orig_s = s_t1\n",
    "#         s_t1 = norm(s_t1)\n",
    "        D.append((s_t, a, r, s_t1, done))\n",
    "        s_t = s_t1\n",
    "        reward += r\n",
    "        if done:\n",
    "#             print s_t\n",
    "#             print orig_s\n",
    "            epsilon -= (initial_epsilon - final_epsilon) / 100\n",
    "            print reward, epsilon\n",
    "            s_t = env.reset()\n",
    "    for j in range(100):\n",
    "        now_index = np.random.randint(len(D), size=1)[0]\n",
    "        s_t, action, r, s_t1, done = D[now_index]\n",
    "        x = s_t1        \n",
    "        x = s_t1\n",
    "        y = achieve_y(r, x, done, Mn, ALD_buf)\n",
    "        k_tt = Guassion_kernel(x, x)\n",
    "        a_t = np.dot(K_inv, k_t)\n",
    "        delta_t = k_tt - np.dot(k_t.T, a_t)\n",
    "        if abs(delta_t) > mu:\n",
    "            ALD_buf.append(x)\n",
    "            ac_t = a_t\n",
    "            a_t = np.matrix(np.zeros(len(ALD_buf))).T\n",
    "            a_t[-1, -1] = 1\n",
    "            K_inv = K_inv*delta_t[0,0]+np.dot(ac_t, ac_t.T)\n",
    "            K_inv = np.row_stack((K_inv, -ac_t.T))\n",
    "            add_c = np.row_stack((-ac_t, 1))\n",
    "            K_inv = np.column_stack((K_inv, add_c))/delta_t[0,0]\n",
    "\n",
    "            k_t = np.row_stack((k_t, k_tt))\n",
    "            Pn = np.row_stack((Pn, np.zeros(len(ALD_buf)-1)))\n",
    "            Pn = np.column_stack((Pn, np.zeros(len(ALD_buf))))\n",
    "            Pn[-1, -1] = 1/alpha\n",
    "            qn = beta*np.dot(np.dot(Pn, k_t),k_t.T)/(1+beta*np.dot(np.dot(k_t.T, Pn), k_t))\n",
    "            Pn1 = Pn-np.dot(qn, Pn)\n",
    "            Mn = np.row_stack((Mn, np.zeros(2)))\n",
    "            Mn1 = beta*y*np.dot(Pn1, k_t)+Mn[:,action]-np.dot(qn, Mn[:,action])\n",
    "            Pn = Pn1\n",
    "            Mn[:,action] = Mn1\n",
    "        else:\n",
    "            qn = beta*np.dot(np.dot(Pn, k_t),k_t.T)/(1+beta*np.dot(np.dot(k_t.T, Pn), k_t))\n",
    "            Pn1 = Pn-np.dot(qn, Pn)\n",
    "            Mn1 = beta*y*np.dot(Pn1, k_t)+Mn[:,action]-np.dot(qn, Mn[:,action])\n",
    "            Pn = Pn1\n",
    "            Mn[:,action] = Mn1  \n",
    "            \n",
    "print('is over')\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "import random as ran\n",
    "import matplotlib.pyplot as plt\n",
    "ran.seed(0)\n",
    "\n",
    "class Mdp:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.states         = [1,2,3,4,5,6,7,8] # 0 indicates end\n",
    "        self.terminal_states      = dict()\n",
    "        self.terminal_states[6]   = 1\n",
    "        self.terminal_states[7]   = 1\n",
    "        self.terminal_states[8]   = 1\n",
    "\n",
    "        self.actions        = ['n','e','s','w']\n",
    "\n",
    "        self.rewards        = dict();\n",
    "        self.rewards['1_s'] = -1.0\n",
    "        self.rewards['3_s'] = 1.0\n",
    "        self.rewards['5_s'] = -1.0\n",
    "\n",
    "        self.t              = dict();\n",
    "        self.t['1_s']       = 6\n",
    "        self.t['1_e']       = 2\n",
    "        self.t['2_w']       = 1\n",
    "        self.t['2_e']       = 3\n",
    "        self.t['3_s']       = 7\n",
    "        self.t['3_w']       = 2\n",
    "        self.t['3_e']       = 4\n",
    "        self.t['4_w']       = 3\n",
    "        self.t['4_e']       = 5\n",
    "        self.t['5_s']       = 8 \n",
    "        self.t['5_w']       = 4\n",
    "\n",
    "        self.gamma          = 0.8\n",
    "\n",
    "    def transform(self, state, action): ##return is_terminal,state, reward\n",
    "        if state in self.terminal_states:\n",
    "            return True, state, 0\n",
    "\n",
    "        key = '%d_%s'%(state, action)\n",
    "        if key in self.t: \n",
    "            next_state = self.t[key]\n",
    "        else:\n",
    "            next_state = state       \n",
    " \n",
    "        is_terminal = False\n",
    "        if next_state in self.terminal_states:\n",
    "            is_terminal = True\n",
    "      \n",
    "        if key not in self.rewards:\n",
    "            r = 0.0\n",
    "        else:\n",
    "            r = self.rewards[key]\n",
    "           \n",
    "        return is_terminal, next_state, r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def random_pi():\n",
    "    actions = ['n','w','e','s']\n",
    "    r       = int(ran.random() * 4)\n",
    "    return actions[r]\n",
    "\n",
    "def compute_random_pi_state_value():\n",
    "    value = [ 0.0 for r in xrange(9)]\n",
    "    num   = 1000000\n",
    "\n",
    "    for k in xrange(1,num):\n",
    "        for i in xrange(1,6):       \n",
    "            mdp = Mdp();\n",
    "            s   = i;\n",
    "            is_terminal = False\n",
    "            gamma = 1.0\n",
    "            v     = 0.0\n",
    "            while False == is_terminal:\n",
    "                a                 = random_pi()\n",
    "                is_terminal, s, r = mdp.transform(s, a)\n",
    "                v                += gamma * r;\n",
    "                gamma            *= 0.5\n",
    "  \n",
    "            value[i] = (value[i] * (k-1) + v) / k\n",
    "\n",
    "        if k % 10000 == 0:\n",
    "            plt.plot(value)\n",
    "#             print value\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "compute_random_pi_state_value()\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
