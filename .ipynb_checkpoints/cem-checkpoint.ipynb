{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-14 22:56:58,271] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0. Episode mean reward:  20.455\n",
      "Iteration  1. Episode mean reward:  53.888\n",
      "Iteration  2. Episode mean reward: 107.483\n",
      "Iteration  3. Episode mean reward: 161.607\n",
      "Iteration  4. Episode mean reward: 184.425\n",
      "Iteration  5. Episode mean reward: 191.695\n",
      "Iteration  6. Episode mean reward: 193.685\n",
      "Iteration  7. Episode mean reward: 195.037\n",
      "Iteration  8. Episode mean reward: 195.692\n",
      "Iteration  9. Episode mean reward: 197.135\n",
      "Iteration 10. Episode mean reward: 199.030\n",
      "Iteration 11. Episode mean reward: 197.393\n",
      "Iteration 12. Episode mean reward: 199.072\n",
      "Iteration 13. Episode mean reward: 199.218\n",
      "Iteration 14. Episode mean reward: 199.882\n",
      "Iteration 15. Episode mean reward: 199.905\n",
      "Iteration 16. Episode mean reward: 199.028\n",
      "Iteration 17. Episode mean reward: 199.610\n",
      "Iteration 18. Episode mean reward: 199.898\n",
      "Iteration 19. Episode mean reward: 199.755\n",
      "Iteration 20. Episode mean reward: 199.815\n",
      "Iteration 21. Episode mean reward: 199.895\n",
      "Iteration 22. Episode mean reward: 199.820\n",
      "Iteration 23. Episode mean reward: 199.993\n",
      "Iteration 24. Episode mean reward: 199.940\n",
      "Iteration 25. Episode mean reward: 199.912\n",
      "Iteration 26. Episode mean reward: 199.887\n",
      "Iteration 27. Episode mean reward: 199.985\n",
      "Iteration 28. Episode mean reward: 200.000\n",
      "Iteration 29. Episode mean reward: 199.977\n",
      "Iteration 30. Episode mean reward: 199.525\n",
      "Iteration 31. Episode mean reward: 200.000\n",
      "Iteration 32. Episode mean reward: 200.000\n",
      "Iteration 33. Episode mean reward: 200.000\n",
      "Iteration 34. Episode mean reward: 199.947\n",
      "Iteration 35. Episode mean reward: 200.000\n",
      "Iteration 36. Episode mean reward: 200.000\n",
      "Iteration 37. Episode mean reward: 200.000\n",
      "Iteration 38. Episode mean reward: 200.000\n",
      "Iteration 39. Episode mean reward: 200.000\n",
      "Iteration 40. Episode mean reward: 200.000\n",
      "Iteration 41. Episode mean reward: 200.000\n",
      "Iteration 42. Episode mean reward: 199.963\n",
      "Iteration 43. Episode mean reward: 199.805\n",
      "Iteration 44. Episode mean reward: 199.988\n",
      "Iteration 45. Episode mean reward: 200.000\n",
      "Iteration 46. Episode mean reward: 199.900\n",
      "Iteration 47. Episode mean reward: 199.995\n",
      "Iteration 48. Episode mean reward: 200.000\n",
      "Iteration 49. Episode mean reward: 199.860\n",
      "Iteration 50. Episode mean reward: 199.998\n",
      "Iteration 51. Episode mean reward: 199.688\n",
      "Iteration 52. Episode mean reward: 199.780\n",
      "Iteration 53. Episode mean reward: 199.920\n",
      "Iteration 54. Episode mean reward: 199.825\n",
      "Iteration 55. Episode mean reward: 199.955\n",
      "Iteration 56. Episode mean reward: 199.982\n",
      "Iteration 57. Episode mean reward: 200.000\n",
      "Iteration 58. Episode mean reward: 200.000\n",
      "Iteration 59. Episode mean reward: 199.980\n",
      "Iteration 60. Episode mean reward: 199.952\n",
      "Iteration 61. Episode mean reward: 200.000\n",
      "Iteration 62. Episode mean reward: 200.000\n",
      "Iteration 63. Episode mean reward: 199.825\n",
      "Iteration 64. Episode mean reward: 199.935\n",
      "Iteration 65. Episode mean reward: 199.998\n",
      "Iteration 66. Episode mean reward: 199.963\n",
      "Iteration 67. Episode mean reward: 200.000\n",
      "Iteration 68. Episode mean reward: 199.803\n",
      "Iteration 69. Episode mean reward: 200.000\n",
      "Iteration 70. Episode mean reward: 199.938\n",
      "Iteration 71. Episode mean reward: 200.000\n",
      "Iteration 72. Episode mean reward: 200.000\n",
      "Iteration 73. Episode mean reward: 200.000\n",
      "Iteration 74. Episode mean reward: 200.000\n",
      "Iteration 75. Episode mean reward: 200.000\n",
      "Iteration 76. Episode mean reward: 200.000\n",
      "Iteration 77. Episode mean reward: 200.000\n",
      "Iteration 78. Episode mean reward: 200.000\n",
      "Iteration 79. Episode mean reward: 200.000\n",
      "Iteration 80. Episode mean reward: 200.000\n",
      "Iteration 81. Episode mean reward: 200.000\n",
      "Iteration 82. Episode mean reward: 200.000\n",
      "Iteration 83. Episode mean reward: 200.000\n",
      "Iteration 84. Episode mean reward: 200.000\n",
      "Iteration 85. Episode mean reward: 200.000\n",
      "Iteration 86. Episode mean reward: 200.000\n",
      "Iteration 87. Episode mean reward: 200.000\n",
      "Iteration 88. Episode mean reward: 200.000\n",
      "Iteration 89. Episode mean reward: 200.000\n",
      "Iteration 90. Episode mean reward: 200.000\n",
      "Iteration 91. Episode mean reward: 200.000\n",
      "Iteration 92. Episode mean reward: 200.000\n",
      "Iteration 93. Episode mean reward: 199.977\n",
      "Iteration 94. Episode mean reward: 199.977\n",
      "Iteration 95. Episode mean reward: 200.000\n",
      "Iteration 96. Episode mean reward: 200.000\n",
      "Iteration 97. Episode mean reward: 200.000\n",
      "Iteration 98. Episode mean reward: 200.000\n",
      "Iteration 99. Episode mean reward: 200.000\n",
      "[-0.17677851 -0.8941039  -1.94331048 -1.95996783  0.05254057]\n",
      "stay up time: 3.56194496155\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "class BinaryActionLinearPolicy(object):\n",
    "    def __init__(self, theta):\n",
    "        self.w = theta[:-1]\n",
    "        self.b = theta[-1]\n",
    "    def act(self, ob):\n",
    "        y = ob.dot(self.w) + self.b\n",
    "        a = int(y < 0)\n",
    "        return a\n",
    "\n",
    "class ContinuousActionLinearPolicy(object):\n",
    "    def __init__(self, theta, n_in, n_out):\n",
    "        assert len(theta) == (n_in + 1) * n_out\n",
    "        self.W = theta[0 : n_in * n_out].reshape(n_in, n_out)\n",
    "        self.b = theta[n_in * n_out : None].reshape(1, n_out)\n",
    "    def act(self, ob):\n",
    "        a = ob.dot(self.W) + self.b\n",
    "        return a\n",
    "\n",
    "def cem(f, th_mean, batch_size, n_iter, elite_frac, initial_std=1.0):\n",
    "    \"\"\"\n",
    "    Generic implementation of the cross-entropy method for maximizing a black-box function\n",
    "\n",
    "    f: a function mapping from vector -> scalar\n",
    "    th_mean: initial mean over input distribution\n",
    "    batch_size: number of samples of theta to evaluate per batch\n",
    "    n_iter: number of batches\n",
    "    elite_frac: each batch, select this fraction of the top-performing samples\n",
    "    initial_std: initial standard deviation over parameter vectors\n",
    "    \"\"\"\n",
    "    n_elite = int(np.round(batch_size*elite_frac))\n",
    "    th_std = np.ones_like(th_mean) * initial_std\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        ths = np.array([th_mean + dth for dth in  th_std[None,:]*np.random.randn(batch_size, th_mean.size)])\n",
    "        ys = np.array([f(th) for th in ths])\n",
    "        elite_inds = ys.argsort()[::-1][:n_elite]\n",
    "        elite_ths = ths[elite_inds]\n",
    "        th_mean = elite_ths.mean(axis=0)\n",
    "        th_std = elite_ths.std(axis=0)\n",
    "        yield {'ys' : ys, 'theta_mean' : th_mean, 'y_mean' : ys.mean()}\n",
    "        \n",
    "        \n",
    "def do_rollout(agent, env, num_steps, render=False):\n",
    "    total_rew = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(num_steps):\n",
    "        a = agent.act(ob)\n",
    "        (ob, reward, done, _info) = env.step(a)\n",
    "        total_rew += reward\n",
    "        if render and t%3==0: env.render()\n",
    "        if done: break\n",
    "    return total_rew, t+1\n",
    "\n",
    "def noisy_evaluation(theta):\n",
    "    agent = BinaryActionLinearPolicy(theta)\n",
    "    rew, T = do_rollout(agent, env, 200)\n",
    "    return rew\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "params = dict(n_iter=100, batch_size=400, elite_frac = 0.2)\n",
    "num_steps = 200\n",
    "\n",
    "for (i, iterdata) in enumerate(\n",
    "    cem(noisy_evaluation, np.zeros(env.observation_space.shape[0]+1), **params)):\n",
    "    print('Iteration %2i. Episode mean reward: %7.3f'%(i, iterdata['y_mean']))\n",
    "    agent = BinaryActionLinearPolicy(iterdata['theta_mean'])\n",
    "\n",
    "print iterdata['theta_mean']\n",
    "ob = env.reset()\n",
    "done = False\n",
    "start_t = time.time()\n",
    "while done == False:\n",
    "    a = agent.act(ob)\n",
    "    (ob, reward, done, _info) = env.step(a)\n",
    "    env.render()\n",
    "    if done: break\n",
    "end_t = time.time()    \n",
    "print('stay up time: '+str(end_t-start_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
