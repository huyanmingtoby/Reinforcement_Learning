{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-14 22:20:11,245] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0. Episode mean reward:  25.640\n",
      "Iteration  1. Episode mean reward:  86.040\n",
      "Iteration  2. Episode mean reward: 171.120\n",
      "Iteration  3. Episode mean reward: 196.080\n",
      "Iteration  4. Episode mean reward: 198.480\n",
      "Iteration  5. Episode mean reward: 200.000\n",
      "Iteration  6. Episode mean reward: 200.000\n",
      "Iteration  7. Episode mean reward: 200.000\n",
      "Iteration  8. Episode mean reward: 200.000\n",
      "Iteration  9. Episode mean reward: 200.000\n",
      "[-0.21987545 -0.58686344 -0.93726108 -3.13897467 -0.11950474]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class BinaryActionLinearPolicy(object):\n",
    "    def __init__(self, theta):\n",
    "        self.w = theta[:-1]\n",
    "        self.b = theta[-1]\n",
    "    def act(self, ob):\n",
    "        y = ob.dot(self.w) + self.b\n",
    "        a = int(y < 0)\n",
    "        return a\n",
    "\n",
    "class ContinuousActionLinearPolicy(object):\n",
    "    def __init__(self, theta, n_in, n_out):\n",
    "        assert len(theta) == (n_in + 1) * n_out\n",
    "        self.W = theta[0 : n_in * n_out].reshape(n_in, n_out)\n",
    "        self.b = theta[n_in * n_out : None].reshape(1, n_out)\n",
    "    def act(self, ob):\n",
    "        a = ob.dot(self.W) + self.b\n",
    "        return a\n",
    "\n",
    "def cem(f, th_mean, batch_size, n_iter, elite_frac, initial_std=1.0):\n",
    "    \"\"\"\n",
    "    Generic implementation of the cross-entropy method for maximizing a black-box function\n",
    "\n",
    "    f: a function mapping from vector -> scalar\n",
    "    th_mean: initial mean over input distribution\n",
    "    batch_size: number of samples of theta to evaluate per batch\n",
    "    n_iter: number of batches\n",
    "    elite_frac: each batch, select this fraction of the top-performing samples\n",
    "    initial_std: initial standard deviation over parameter vectors\n",
    "    \"\"\"\n",
    "    n_elite = int(np.round(batch_size*elite_frac))\n",
    "    th_std = np.ones_like(th_mean) * initial_std\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        ths = np.array([th_mean + dth for dth in  th_std[None,:]*np.random.randn(batch_size, th_mean.size)])\n",
    "        ys = np.array([f(th) for th in ths])\n",
    "        elite_inds = ys.argsort()[::-1][:n_elite]\n",
    "        elite_ths = ths[elite_inds]\n",
    "        th_mean = elite_ths.mean(axis=0)\n",
    "        th_std = elite_ths.std(axis=0)\n",
    "        yield {'ys' : ys, 'theta_mean' : th_mean, 'y_mean' : ys.mean()}\n",
    "        \n",
    "        \n",
    "def do_rollout(agent, env, num_steps, render=False):\n",
    "    total_rew = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(num_steps):\n",
    "        a = agent.act(ob)\n",
    "        (ob, reward, done, _info) = env.step(a)\n",
    "        total_rew += reward\n",
    "        if render and t%3==0: env.render()\n",
    "        if done: break\n",
    "    return total_rew, t+1\n",
    "\n",
    "def noisy_evaluation(theta):\n",
    "    agent = BinaryActionLinearPolicy(theta)\n",
    "    rew, T = do_rollout(agent, env, num_steps)\n",
    "    return rew\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "params = dict(n_iter=10, batch_size=25, elite_frac = 0.2)\n",
    "num_steps = 200\n",
    "\n",
    "for (i, iterdata) in enumerate(\n",
    "    cem(noisy_evaluation, np.zeros(env.observation_space.shape[0]+1), **params)):\n",
    "    print('Iteration %2i. Episode mean reward: %7.3f'%(i, iterdata['y_mean']))\n",
    "    agent = BinaryActionLinearPolicy(iterdata['theta_mean'])\n",
    "\n",
    "print iterdata['theta_mean']\n",
    "ob = env.reset()\n",
    "done = False\n",
    "while done == False:\n",
    "    a = agent.act(ob)\n",
    "    (ob, reward, done, _info) = env.step(a)\n",
    "    env.render()\n",
    "    if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
