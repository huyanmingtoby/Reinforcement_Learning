{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": null,
>>>>>>> ae9eb22e4245bf3e600c7f9c28c2c1ac3852f3ce
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2017-06-14 20:57:07,631] Making new env: CartPole-v0\n"
=======
      "[2017-06-14 19:52:58,958] Making new env: CartPole-v0\n"
>>>>>>> ae9eb22e4245bf3e600c7f9c28c2c1ac3852f3ce
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Iteration  0. Episode mean reward:  20.114\n",
      "Iteration  1. Episode mean reward:  51.366\n",
      "Iteration  2. Episode mean reward:  96.418\n",
      "Iteration  3. Episode mean reward: 149.431\n",
      "Iteration  4. Episode mean reward: 175.590\n",
      "Iteration  5. Episode mean reward: 185.031\n",
      "Iteration  6. Episode mean reward: 190.478\n",
      "Iteration  7. Episode mean reward: 193.804\n",
      "Iteration  8. Episode mean reward: 195.813\n",
      "Iteration  9. Episode mean reward: 196.700\n",
      "Iteration 10. Episode mean reward: 197.516\n",
      "Iteration 11. Episode mean reward: 197.903\n",
      "Iteration 12. Episode mean reward: 198.728\n",
      "Iteration 13. Episode mean reward: 199.158\n",
      "Iteration 14. Episode mean reward: 198.774\n",
      "Iteration 15. Episode mean reward: 199.161\n",
      "Iteration 16. Episode mean reward: 198.519\n",
      "Iteration 17. Episode mean reward: 199.468\n",
      "Iteration 18. Episode mean reward: 199.527\n",
      "Iteration 19. Episode mean reward: 199.608\n",
      "Iteration 20. Episode mean reward: 199.300\n",
      "Iteration 21. Episode mean reward: 199.712\n",
      "Iteration 22. Episode mean reward: 199.593\n",
      "Iteration 23. Episode mean reward: 199.712\n",
      "Iteration 24. Episode mean reward: 199.685\n",
      "Iteration 25. Episode mean reward: 199.739\n",
      "Iteration 26. Episode mean reward: 199.790\n",
      "Iteration 27. Episode mean reward: 199.752\n",
      "Iteration 28. Episode mean reward: 199.928\n",
      "Iteration 29. Episode mean reward: 199.889\n",
      "Iteration 30. Episode mean reward: 199.816\n",
      "Iteration 31. Episode mean reward: 199.855\n",
      "Iteration 32. Episode mean reward: 199.892\n",
      "Iteration 33. Episode mean reward: 199.804\n",
      "Iteration 34. Episode mean reward: 199.942\n",
      "Iteration 35. Episode mean reward: 199.998\n",
      "Iteration 36. Episode mean reward: 199.880\n",
      "Iteration 37. Episode mean reward: 199.880\n",
      "Iteration 38. Episode mean reward: 199.937\n",
      "Iteration 39. Episode mean reward: 199.974\n",
      "Iteration 40. Episode mean reward: 199.917\n",
      "Iteration 41. Episode mean reward: 199.975\n",
      "Iteration 42. Episode mean reward: 199.911\n",
      "Iteration 43. Episode mean reward: 199.968\n",
      "Iteration 44. Episode mean reward: 199.909\n",
      "Iteration 45. Episode mean reward: 199.915\n",
      "Iteration 46. Episode mean reward: 199.877\n",
      "Iteration 47. Episode mean reward: 199.906\n",
      "Iteration 48. Episode mean reward: 199.921\n",
      "Iteration 49. Episode mean reward: 199.893\n",
      "Iteration 50. Episode mean reward: 199.905\n",
      "Iteration 51. Episode mean reward: 199.962\n",
      "Iteration 52. Episode mean reward: 199.907\n",
      "Iteration 53. Episode mean reward: 199.941\n",
      "Iteration 54. Episode mean reward: 199.942\n",
      "Iteration 55. Episode mean reward: 199.981\n",
      "Iteration 56. Episode mean reward: 199.933\n",
      "Iteration 57. Episode mean reward: 199.999\n",
      "Iteration 58. Episode mean reward: 199.944\n",
      "Iteration 59. Episode mean reward: 199.967\n",
      "Iteration 60. Episode mean reward: 199.946\n",
      "Iteration 61. Episode mean reward: 199.906\n",
      "Iteration 62. Episode mean reward: 199.953\n",
      "Iteration 63. Episode mean reward: 199.986\n",
      "Iteration 64. Episode mean reward: 199.955\n",
      "Iteration 65. Episode mean reward: 199.968\n",
      "Iteration 66. Episode mean reward: 199.954\n",
      "Iteration 67. Episode mean reward: 199.969\n",
      "Iteration 68. Episode mean reward: 199.990\n",
      "Iteration 69. Episode mean reward: 199.964\n",
      "Iteration 70. Episode mean reward: 199.992\n",
      "Iteration 71. Episode mean reward: 199.998\n",
      "Iteration 72. Episode mean reward: 199.952\n",
      "Iteration 73. Episode mean reward: 199.990\n",
      "Iteration 74. Episode mean reward: 199.999\n",
      "Iteration 75. Episode mean reward: 199.913\n",
      "Iteration 76. Episode mean reward: 199.983\n",
      "Iteration 77. Episode mean reward: 199.953\n",
      "Iteration 78. Episode mean reward: 199.926\n",
      "Iteration 79. Episode mean reward: 199.992\n",
      "Iteration 80. Episode mean reward: 200.000\n",
      "Iteration 81. Episode mean reward: 199.993\n",
      "Iteration 82. Episode mean reward: 199.984\n",
      "Iteration 83. Episode mean reward: 199.980\n",
      "Iteration 84. Episode mean reward: 199.962\n",
      "Iteration 85. Episode mean reward: 199.980\n",
      "Iteration 86. Episode mean reward: 199.958\n",
      "Iteration 87. Episode mean reward: 199.971\n",
      "Iteration 88. Episode mean reward: 199.976\n",
      "Iteration 89. Episode mean reward: 199.990\n",
      "Iteration 90. Episode mean reward: 199.985\n",
      "Iteration 91. Episode mean reward: 199.992\n",
      "Iteration 92. Episode mean reward: 199.993\n",
      "Iteration 93. Episode mean reward: 199.970\n",
      "Iteration 94. Episode mean reward: 199.976\n",
      "Iteration 95. Episode mean reward: 199.986\n",
      "Iteration 96. Episode mean reward: 199.981\n",
      "Iteration 97. Episode mean reward: 199.995\n",
      "Iteration 98. Episode mean reward: 200.000\n",
      "Iteration 99. Episode mean reward: 199.976\n",
      "[ 0.0085582  -0.94520594 -1.8430938  -1.85504143  0.02609372]\n"
=======
      "Iteration  0. Episode mean reward:  18.452\n",
      "Iteration  1. Episode mean reward:  46.037\n",
      "Iteration  2. Episode mean reward:  88.221\n",
      "Iteration  3. Episode mean reward: 137.528\n",
      "Iteration  4. Episode mean reward: 169.064\n",
      "Iteration  5. Episode mean reward: 183.111\n",
      "Iteration  6. Episode mean reward: 189.869\n",
      "Iteration  7. Episode mean reward: 192.524\n",
      "Iteration  8. Episode mean reward: 194.647\n",
      "Iteration  9. Episode mean reward: 196.138\n",
      "Iteration 10. Episode mean reward: 197.085\n",
      "Iteration 11. Episode mean reward: 197.865\n",
      "Iteration 12. Episode mean reward: 197.958\n",
      "Iteration 13. Episode mean reward: 198.268\n",
      "Iteration 14. Episode mean reward: 198.671\n",
      "Iteration 15. Episode mean reward: 198.868\n",
      "Iteration 16. Episode mean reward: 198.915\n",
      "Iteration 17. Episode mean reward: 199.192\n",
      "Iteration 18. Episode mean reward: 199.271\n",
      "Iteration 19. Episode mean reward: 199.282\n",
      "Iteration 20. Episode mean reward: 199.318\n",
      "Iteration 21. Episode mean reward: 199.558\n",
      "Iteration 22. Episode mean reward: 199.541\n",
      "Iteration 23. Episode mean reward: 199.515\n",
      "Iteration 24. Episode mean reward: 199.509\n",
      "Iteration 25. Episode mean reward: 199.513\n",
      "Iteration 26. Episode mean reward: 199.639\n",
      "Iteration 27. Episode mean reward: 199.601\n",
      "Iteration 28. Episode mean reward: 199.597\n",
      "Iteration 29. Episode mean reward: 199.633\n",
      "Iteration 30. Episode mean reward: 199.724\n",
      "Iteration 31. Episode mean reward: 199.659\n",
      "Iteration 32. Episode mean reward: 199.738\n",
      "Iteration 33. Episode mean reward: 199.702\n",
      "Iteration 34. Episode mean reward: 199.722\n",
      "Iteration 35. Episode mean reward: 199.775\n",
      "Iteration 36. Episode mean reward: 199.753\n",
      "Iteration 37. Episode mean reward: 199.816\n",
      "Iteration 38. Episode mean reward: 199.830\n",
      "Iteration 39. Episode mean reward: 199.777\n",
      "Iteration 40. Episode mean reward: 199.798\n",
      "Iteration 41. Episode mean reward: 199.823\n",
      "Iteration 42. Episode mean reward: 199.793\n",
      "Iteration 43. Episode mean reward: 199.792\n",
      "Iteration 44. Episode mean reward: 199.767\n",
      "Iteration 45. Episode mean reward: 199.791\n",
      "Iteration 46. Episode mean reward: 199.806\n",
      "Iteration 47. Episode mean reward: 199.781\n",
      "Iteration 48. Episode mean reward: 199.847\n",
      "Iteration 49. Episode mean reward: 199.809\n",
      "Iteration 50. Episode mean reward: 199.846\n",
      "Iteration 51. Episode mean reward: 199.818\n",
      "Iteration 52. Episode mean reward: 199.854\n",
      "Iteration 53. Episode mean reward: 199.793\n",
      "Iteration 54. Episode mean reward: 199.805\n",
      "Iteration 55. Episode mean reward: 199.815\n",
      "Iteration 56. Episode mean reward: 199.846\n",
      "Iteration 57. Episode mean reward: 199.874\n",
      "Iteration 58. Episode mean reward: 199.888\n",
      "Iteration 59. Episode mean reward: 199.862\n",
      "Iteration 60. Episode mean reward: 199.896\n",
      "Iteration 61. Episode mean reward: 199.862\n",
      "Iteration 62. Episode mean reward: 199.867\n",
      "Iteration 63. Episode mean reward: 199.877\n",
      "Iteration 64. Episode mean reward: 199.896\n",
      "Iteration 65. Episode mean reward: 199.865\n",
      "Iteration 66. Episode mean reward: 199.911\n",
      "Iteration 67. Episode mean reward: 199.927\n",
      "Iteration 68. Episode mean reward: 199.903\n",
      "Iteration 69. Episode mean reward: 199.895\n",
      "Iteration 70. Episode mean reward: 199.894\n",
      "Iteration 71. Episode mean reward: 199.842\n",
      "Iteration 72. Episode mean reward: 199.898\n",
      "Iteration 73. Episode mean reward: 199.930\n",
      "Iteration 74. Episode mean reward: 199.868\n",
      "Iteration 75. Episode mean reward: 199.884\n",
      "Iteration 76. Episode mean reward: 199.869\n",
      "Iteration 77. Episode mean reward: 199.880\n",
      "Iteration 78. Episode mean reward: 199.889\n",
      "Iteration 79. Episode mean reward: 199.901\n",
      "Iteration 80. Episode mean reward: 199.868\n",
      "Iteration 81. Episode mean reward: 199.909\n",
      "Iteration 82. Episode mean reward: 199.919\n",
      "Iteration 83. Episode mean reward: 199.910\n",
      "Iteration 84. Episode mean reward: 199.867\n",
      "Iteration 85. Episode mean reward: 199.954\n",
      "Iteration 86. Episode mean reward: 199.891\n",
      "Iteration 87. Episode mean reward: 199.930\n",
      "Iteration 88. Episode mean reward: 199.907\n",
      "Iteration 89. Episode mean reward: 199.867\n"
>>>>>>> ae9eb22e4245bf3e600c7f9c28c2c1ac3852f3ce
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "class BinaryActionLinearPolicy(object):\n",
    "    def __init__(self, theta):\n",
    "        self.w = theta[:-1]\n",
    "        self.b = theta[-1]\n",
    "    def act(self, ob):\n",
    "        y = ob.dot(self.w) + self.b\n",
    "        a = int(y < 0)\n",
    "        return a\n",
    "\n",
    "class ContinuousActionLinearPolicy(object):\n",
    "    def __init__(self, theta, n_in, n_out):\n",
    "        assert len(theta) == (n_in + 1) * n_out\n",
    "        self.W = theta[0 : n_in * n_out].reshape(n_in, n_out)\n",
    "        self.b = theta[n_in * n_out : None].reshape(1, n_out)\n",
    "    def act(self, ob):\n",
    "        a = ob.dot(self.W) + self.b\n",
    "        return a\n",
    "\n",
    "\n",
    "def cem(f, th_mean, batch_size, n_iter, elite_frac, initial_std=1.0):\n",
    "    \"\"\"\n",
    "    Generic implementation of the cross-entropy method for maximizing a black-box function\n",
    "    \n",
    "    f:          a function mapping from vector -> scalar\n",
    "    th_mean:    initial mean over input distribution\n",
    "    batch_size: number of samples of theta to evaluate per batch\n",
    "    n_iter:     number of batches\n",
    "    elite_frac: each batch, select this fraction of the top-performing samples\n",
    "    initial_std:initial standard deviation over parameter vectors\n",
    "    \"\"\"\n",
    "    n_elite = int(np.round(batch_size*elite_frac))\n",
    "    th_std = np.ones_like(th_mean)*initial_std\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        ths = np.array([th_mean + dth for dth in th_std[None, :]*np.random.randn(batch_size, th_mean.size)])\n",
    "        ys = np.array([f(th) for th in ths])\n",
    "        elite_inds = ys.argsort()[::-1][:n_elite] #先从小到大排序，然后逆序，再选取前面n_elite个元素\n",
    "        elite_ths = ths[elite_inds]\n",
    "        th_mean = elite_ths.mean(axis=0)\n",
    "        th_std = elite_ths.std(axis=0)\n",
    "        yield {'ys' : ys, 'theta_mean' : th_mean, 'y_mean' : ys.mean()}\n",
    "        \n",
    "def do_rollout(agent, env, num_steps, render=False):\n",
    "    total_rew = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(num_steps):\n",
    "        a = agent.act(ob)\n",
    "        (ob, reward, done, _info) = env.step(a)\n",
    "        total_rew += reward\n",
    "        if render and t%3==0: env.render()\n",
    "        if done: break\n",
    "    return total_rew, t+1\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
<<<<<<< HEAD
    "params = dict(n_iter=100, batch_size=2500, elite_frac=0.2)\n",
    "num_steps = 300\n",
=======
    "params = dict(n_iter=10000, batch_size=25000, elite_frac=0.2)\n",
    "num_steps = 400\n",
>>>>>>> ae9eb22e4245bf3e600c7f9c28c2c1ac3852f3ce
    "\n",
    "# outdir = '/tmp/cem-agent-results'\n",
    "# env = wrappers.Monitor(env, outdir, force=True)\n",
    "\n",
    "def noisy_evaluation(theta):\n",
    "    agent = BinaryActionLinearPolicy(theta)\n",
    "    rew, T = do_rollout(agent, env, num_steps)\n",
    "    return rew\n",
    "\n",
    "for (i, iterdata) in enumerate(cem(noisy_evaluation, np.zeros(env.observation_space.shape[0]+1), **params)):\n",
    "    print('Iteration %2i. Episode mean reward: %7.3f'%(i, iterdata['y_mean']))\n",
    "    agent = BinaryActionLinearPolicy(iterdata['theta_mean'])\n",
    "    \n",
    "\n",
    "print iterdata['theta_mean']\n",
    "\n",
    "ob = env.reset()\n",
    "for i in range(100):\n",
    "    a = agent.act(ob)\n",
    "    (ob, reward, done, _info) = env.step(a)\n",
    "    if i%3==0: env.render()\n",
    "    if done: ob = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
